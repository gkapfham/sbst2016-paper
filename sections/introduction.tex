% vim: ft=tex
%!TEX root=sbst2016.tex

\section{Introduction}
\label{sec:introduction}

The field of search-based software testing (SBST) often involves the implementation and experimental evaluation of
algorithms that employ randomization. For instance, automated test data generation (ATDG) with the alternating variable
method, or \AVM, employs randomness when it restarts after not finding data that successfully meets the chosen
testing objectives~\cite{McMinn2015}. Or, a genetic algorithm performing automated test suite prioritisation (ATSP) that
reorders tests during regression testing will randomly mutate portions of a candidate test suite in order to
create the best ordering~\cite{Walcott2006}.

Scientists must carefully design and conduct the experiments evaluating these algorithms to ensure that they account for
any inherent randomness. It is additionally important that these scientists employ the right methods to analyse the results
from these experiments. In the year 2011, Arcuri and Briand published a conference paper outlining some practical
guidelines for using statistical methods to analyse randomized algorithms~\cite{Arcuri2011}, like those often used in
SBST. The journal version of this paper, entitled ``A Hitchhiker's Guide to Statistical Tests for
Assessing Randomized Algorithms in Software Engineering''~\cite{Arcuri2014}, expands on the earlier paper while still
providing practical ways to rigorously analysis empirical results.

It is hard to underestimate the ways in which these two papers have benefited the SBST community. For instance, many
SBST papers now correctly use non-parametric statistical tests, like the \wilcoxon, to perform hypothesis testing. To
complement these significance tests, many SBST researchers use the nonparametric \atwelve statistic of Vargha and
Delaney \cite{Vargha2000} to compute effect sizes, which determine the average probability that one approach ``out
performs'' another. While these papers have achieved their laudable objectives, in this paper we argue that the
subtleties of the various statistical analysis might cause well-intentioned SBST researchers to make mistakes that
compromise the validity of their empirical results. To this end, we argue that for the improvement of methodological
maturity in the SBST field through the development and use of share repositories of statistical code. That is, we note
that the members of the SBST community --- hitchhikers that we are --- need vehicles to ensure that we attain to higher
levels of maturity in our statistical analyses.

