% vim: ft=tex
%!TEX root=sbst2016.tex

\section{Introduction}
\label{sec:introduction}

The field of search-based software testing (SBST) often involves the implementation and experimental evaluation of
algorithms that employ randomization. For instance, automated test data generation (ATDG) with the alternating variable
method, or \AVM, employs randomness when it restarts after not finding data that successfully meets the chosen
testing objectives~\cite{McMinn2015}. Or, a genetic algorithm performing automated test suite prioritisation (ATSP) that
reorders tests during regression testing will randomly mutate portions of a candidate test suite to aid in
finding the best ordering~\cite{Walcott2006}.

Scientists must carefully design and conduct the experiments evaluating these algorithms to ensure that they account for
any inherent randomness. It is additionally important that these individuals employ the right methods to analyze the
results from these experiments. In the year 2011, Arcuri and Briand published a conference paper outlining some
practical guidelines for using statistical methods to analyze randomized algorithms~\cite{Arcuri2011}, like those often
used in SBST. The journal version of this paper, entitled ``A Hitchhiker's Guide to Statistical Tests for Assessing
Randomized Algorithms in Software Engineering''~\cite{Arcuri2014}, expands on the earlier paper by further explaining
ways to rigorously analyze empirical results.

It is hard to underestimate the ways in which these two papers have benefited the SBST community. For instance, many
SBST researchers now correctly use the non-parametric \wilcoxon to perform hypothesis testing. To complement these
significance tests, many individuals in the field use the nonparametric \atwelve statistic of Vargha and Delaney
\cite{Vargha2000} to compute effect sizes, thereby determining the average probability that one approach out performs
another. While these two papers have achieved laudable ends, we argue that the subtleties of various statistical
analyses might cause well-intentioned SBST researchers to make mistakes that compromise the validity of their empirical
results. To this end, we advocate for the enhancement of methodological maturity in the SBST field through the development
and use of share repositories of statistical code. That is, we note that the members of the SBST community ---
``hitchhikers'' that we are --- need code ``vehicles'' to ensure that we attain to greater levels of correctness in our
statistical analyses.

